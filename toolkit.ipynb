{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text Extraction\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
    "from pdfminer.converter import TextConverter\n",
    "from pdfminer.layout import LAParams\n",
    "import io\n",
    "from docx2python import docx2python\n",
    "from bs4 import BeautifulSoup\n",
    "# -------------------------------------------------\n",
    "\n",
    "# NER\n",
    "import spacy\n",
    "# -------------------------------------------------\n",
    "\n",
    "# O-NET\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import pandas as pd\n",
    "# --------------------------------------------------\n",
    "\n",
    "# Project Description\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tag import pos_tag\n",
    "\n",
    "import re\n",
    "from typing import List, Any\n",
    "import warnings\n",
    "import os\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading averaged_perceptron_tagger: <urlopen error\n",
      "[nltk_data]     [Errno 11001] getaddrinfo failed>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "try:\n",
    "    nltk.download(\"averaged_perceptron_tagger\")\n",
    "    spacy.load(\"en_core_web_sm\")\n",
    "except:\n",
    "    # !python -m spacy download en_core_web_md\n",
    "    !python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def duration(func : Any) -> Any:\n",
    "    @wraps(func)\n",
    "    def wrapper(*args, **kwargs) -> Any:\n",
    "        start_time = time.time()\n",
    "        result = func(*args, **kwargs)\n",
    "        end_time = time.time()\n",
    "        duration = end_time - start_time\n",
    "        print(f\"Function {func.__name__} took {duration:.4f} seconds\")\n",
    "        return result\n",
    "\n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class File2Text:\n",
    "    \"\"\"\n",
    "    A class to extract text content from various types of files such as PDFs, DOCX, and HTML files.\n",
    "\n",
    "    Attributes:\n",
    "        np_of_pages (int): The number of pages in the file.\n",
    "        text (str): The extracted text from the file.\n",
    "        file_path (str): The path to the file.\n",
    "        file_type (str): The type of the file (e.g., \"pdf\", \"docx\", \"html\").\n",
    "\n",
    "    Methods:\n",
    "        __init__(file_path):\n",
    "            Initializes the File2Text object with the given file path.\n",
    "\n",
    "        get_text():\n",
    "            Extracts and returns the text content from the file.\n",
    "\n",
    "        __extractPdfText():\n",
    "            Private method to extract text from a PDF file.\n",
    "\n",
    "        __extractDocx2Text():\n",
    "            Private method to extract text from a DOCX file.\n",
    "\n",
    "        __extractHtml2Text():\n",
    "            Private method to extract text from an HTML file.\n",
    "\n",
    "        __cleanText(text):\n",
    "            Private method to clean and preprocess the extracted text.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, file_path: str) -> None:\n",
    "        \"\"\"\n",
    "        Initializes the File2Text object with the given file path.\n",
    "\n",
    "        Args:\n",
    "            file_path (str): The path to the file.\n",
    "        \"\"\"\n",
    "        self.file_path = file_path\n",
    "        self.file_type = file_path.split(\".\")[-1]\n",
    "\n",
    "    def get_text(self) -> List[str]:\n",
    "        \"\"\"\n",
    "        Extracts and returns the text content from the file.\n",
    "\n",
    "        Returns:\n",
    "            List[str]: A list containing the extracted text.\n",
    "        \"\"\"\n",
    "        __nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "        if self.file_type == \"pdf\":\n",
    "            self.__extractPdfText()\n",
    "        elif self.file_type == \"docx\":\n",
    "            self.__extractDocx2Text()\n",
    "        elif self.file_type == \"html\":\n",
    "            self.__extractHtml2Text()\n",
    "        else:\n",
    "            raise ValueError(\"File type not supported\")\n",
    "        return __nlp(self.__cleanText(self.text)).text\n",
    "\n",
    "    def __extractPdfText(self) -> None:\n",
    "        \"\"\"\n",
    "        Private method to extract text from a PDF file.\n",
    "        \"\"\"\n",
    "        i_f = open(self.file_path, \"rb\")\n",
    "        resMgr = PDFResourceManager()\n",
    "        retData = io.StringIO()\n",
    "        TxtConverter = TextConverter(resMgr, retData, laparams=LAParams())\n",
    "        interpreter = PDFPageInterpreter(resMgr, TxtConverter)\n",
    "        for page in PDFPage.get_pages(i_f):\n",
    "            interpreter.process_page(page)\n",
    "        self.text = retData.getvalue()\n",
    "\n",
    "    def __extractDocx2Text(self) -> None:\n",
    "        \"\"\"\n",
    "        Private method to extract text from a DOCX file.\n",
    "        \"\"\"\n",
    "        self.text = docx2python(self.file_path).text\n",
    "\n",
    "    def __extractHtml2Text(self) -> None:\n",
    "        \"\"\"\n",
    "        Private method to extract text from an HTML file.\n",
    "        \"\"\"\n",
    "        with open(self.file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            html_content = file.read()\n",
    "        soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "        self.text = soup.get_text()\n",
    "\n",
    "    def __cleanText(self, text: str) -> str:\n",
    "        \"\"\"\n",
    "        Private method to clean and preprocess the extracted text.\n",
    "\n",
    "        Args:\n",
    "            text (str): The text to be cleaned.\n",
    "\n",
    "        Returns:\n",
    "            str: The cleaned text.\n",
    "        \"\"\"\n",
    "        # text = text.replace(\"\\n\", \" \")\n",
    "        text = text.replace(\"\\r\", \" \")\n",
    "        # text = text.replace(\"\\t\", \" \").replace(\"\\xa0\", \" \")\n",
    "        text = text.replace(\"\\x0c\", \" \").replace(\"\\x0b\", \" \")\n",
    "        text = text.replace(\"\\x0e\", \" \").replace(\"\\x0f\", \" \").replace(\"\\x1c\", \" \")\n",
    "\n",
    "        text = re.sub(\"http\\S+\\s\", \" \", text)\n",
    "        text = re.sub(\"RT|cc\", \" \", text)\n",
    "        text = re.sub(\"#\\S+\\s\", \" \", text)\n",
    "        text = re.sub(\"@\\S+\", \"  \", text)\n",
    "        text = re.sub(\n",
    "            \"[%s]\" % re.escape(\"\"\"!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\"\"\"), \" \", text\n",
    "        )\n",
    "        text = re.sub(r\"[^\\x00-\\x7f]\", \" \", text)\n",
    "        text = re.sub(\"\\s+\", \" \", text)\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "f2t = File2Text(\"test/Praxal_resume23.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'THARUNESHWAR S itstharuneshwar 91 9524135009 linkedin com in tharuneshwar s EDUCATION MANAKULA VINAYAGAR INSTITUTE OF TECHNOLOGY B Tech in Computer Science 8 2 2021 2025 Puducherry IN KENDRIYA VIDYALAYA HSC 84 2020 2021 Puducherry IN KENDRIYA VIDYALAYA SSLC 75 2018 2019 Puducherry IN SKILLS TECHNICAL SKILLS Github CI CD Firebase Rest API NodeJS FRAMEWORKS LIBRARIES Flask Django React JS PROGRAMMING LANGUAGES Python Java HTML CSS JS SQL GRAPHIC DESIGN Figma Canva CE IFICATIONS MACHINE LEARNING WITH PYTHON IBM NOV 2021 INTRODUCTION TO PYTHON DataCamp NOV 2021 INTERMEDIATE MACHINE LEARNING KAGGLE JAN 2022 FULL STACK DEVELOPEMENT Board Infinity JULY 2023 EXPERIENCE CODEINBOUND Backend Developer Intern JULY 2023 DEC 2023 Delhi IN Developed and implemented a performant API service using Next js and TypeScript TypeScript Nest Js JavaScript PostgreSql HACKER INDIA Backend Engineer AUG 2022 Paper Insert Working under the Backend Team Implemented Firebase for authentication and Firestore for real time data storage Firebase FireStore Authendication Flask JavaScript Analytics Placement Monitoring software Dedicated for College Full Stack Developer JUN 2023 Developing Android app and website for college placement activities Implemented features like coding competitions resume scoring and student job applications Admin panel for staff and mentors to verify and manage results Figma python Flask Django Firebase Flutter LENOVO IN PLANT TRAINING Intern LINK JUN 2023 JULY 2023 Puducherry IN Displayed Professional acumen during the period of In Plant Training in Manufacturing Operations PROJECTS MOVIE VERSE LINK optimal experience Developed user friendly ReactJS app with intuitive interface for Utilized TMDB API to a ess movie database with up to date titles descriptions ratings and images for users Implemented robust search detailed info reviews personalized recommendations for users Sentiment analysis on user reviews using NLP algorithms Naive Bayes Logistic Regression Hosted on Firebase automated deployment via GitHub Actions STREAM SAVER LINK Stream Saver Flask Python web app Download YouTube videos using Pytube library Easy video search and offline saving Simplifies the process of video downloading for users '"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resumeText = f2t.get_text()\n",
    "resumeText"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resume Section\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_project_section(text : str) -> Any:\n",
    "    pattern = r\"(?i)\\b(?:project|developed|implemented|utilized)\\b[\\s\\w,;:]*\"\n",
    "    return re.findall(pattern, resumeText)\n",
    "\n",
    "def extract_education_section(text : str) -> Any:\n",
    "    pattern = r\"(?i)education(?:[\\s\\w,;:()-]*graduated[\\s\\w,;:()-]*:(.*?)(?=(?:[\\s\\w,;:()-]*\\b\\d{4}\\b)|(?:[\\s\\w,;:()-]*$)))\"\n",
    "    education_sections = re.finditer(pattern, resumeText)\n",
    "    return str(education_sections.group())\n",
    "\n",
    "\n",
    "def extract_skills(resumeText):\n",
    "    skills = []\n",
    "\n",
    "    project_descriptions = extract_project_section(resumeText)\n",
    "\n",
    "    for description in project_descriptions:\n",
    "        # Tokenize words and tag parts of speech\n",
    "        words = word_tokenize(description)\n",
    "        tagged_words = pos_tag(words)\n",
    "\n",
    "        # Define grammar for noun phrases\n",
    "        grammar = \"NP: {<JJ>*<NN|NNS>+}\"\n",
    "\n",
    "        # Create a chunk parser with the defined grammar\n",
    "        chunk_parser = nltk.RegexpParser(grammar)\n",
    "        chunked_words = chunk_parser.parse(tagged_words)\n",
    "\n",
    "        # Extract the noun phrases (potential skills)\n",
    "        for subtree in chunked_words.subtrees(filter=lambda t: t.label() == \"NP\"):\n",
    "            skill = \" \".join([word for word, tag in subtree.leaves()])\n",
    "            skills.append(skill)\n",
    "\n",
    "    return list(set(skills))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "CUSTOM_NER_MODEL_PATH = os.path.join(\"models/ner/JdModel/output\", 'model-best')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "resume_ner_model = spacy.load(CUSTOM_NER_MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'transformer': [],\n",
       " 'ner': ['CERTIFICATION', 'DEGREE', 'EXPERIENCE', 'JOBPOST', 'SKILLS']}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resume_ner_model.pipe_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "resume_section = {label.capitalize(): [] for label in resume_ner_model.pipe_labels['ner']}\n",
    "resume_section[\"Projects\"] = (\n",
    "    extract_project_section(resumeText)\n",
    "    if len(extract_project_section(resumeText)) > 0\n",
    "    else []\n",
    ")\n",
    "resume_section[\"Education\"] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = resume_ner_model(resumeText)\n",
    "for ent in doc.ents:\n",
    "    resume_section[ent.label_.capitalize()].append(ent.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(extract_skills(resume_section['Projects'])) == 0:\n",
    "    resume_section[\"Skills\"] = extract_skills(resume_section['Projects'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(len(resume_section[\"Education\"]) != 0):\n",
    "    resume_section[\"Education\"] = extract_education_section(resumeText)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Certification': [],\n",
       " 'Degree': [],\n",
       " 'Experience': [],\n",
       " 'Jobpost': [],\n",
       " 'Skills': ['Github',\n",
       "  'Django',\n",
       "  'Python',\n",
       "  'Java',\n",
       "  'HTML',\n",
       "  'Next',\n",
       "  'js',\n",
       "  'TypeScript',\n",
       "  'Js',\n",
       "  'JavaScript',\n",
       "  'Figma',\n",
       "  'python'],\n",
       " 'Projects': ['Developed and implemented a performant API service using Next js and TypeScript TypeScript Nest Js JavaScript PostgreSql HACKER INDIA Backend Engineer AUG 2022 Paper Insert Working under the Backend Team Implemented Firebase for authentication and Firestore for real time data storage Firebase FireStore Authendication Flask JavaScript Analytics Placement Monitoring software Dedicated for College Full Stack Developer JUN 2023 Developing Android app and website for college placement activities Implemented features like coding competitions resume scoring and student job applications Admin panel for staff and mentors to verify and manage results Figma python Flask Django Firebase Flutter LENOVO IN PLANT TRAINING Intern LINK JUN 2023 JULY 2023 Puducherry IN Displayed Professional acumen during the period of In Plant Training in Manufacturing Operations PROJECTS MOVIE VERSE LINK optimal experience Developed user friendly ReactJS app with intuitive interface for Utilized TMDB API to a ess movie database with up to date titles descriptions ratings and images for users Implemented robust search detailed info reviews personalized recommendations for users Sentiment analysis on user reviews using NLP algorithms Naive Bayes Logistic Regression Hosted on Firebase automated deployment via GitHub Actions STREAM SAVER LINK Stream Saver Flask Python web app Download YouTube videos using Pytube library Easy video search and offline saving Simplifies the process of video downloading for users '],\n",
       " 'Education': []}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resume_section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SKILLS\n",
      "------\n",
      "*  Github\n",
      "*  Django\n",
      "*  Python\n",
      "*  Java\n",
      "*  HTML\n",
      "*  Next\n",
      "*  js\n",
      "*  TypeScript\n",
      "*  Js\n",
      "*  JavaScript\n",
      "*  Figma\n",
      "*  python\n",
      "------------------------------------------------------------\n",
      "PROJECTS\n",
      "--------\n",
      "*  Developed and implemented a performant API service using Next js and TypeScript TypeScript Nest Js JavaScript PostgreSql HACKER INDIA Backend Engineer AUG 2022 Paper Insert Working under the Backend Team Implemented Firebase for authentication and Firestore for real time data storage Firebase FireStore Authendication Flask JavaScript Analytics Placement Monitoring software Dedicated for College Full Stack Developer JUN 2023 Developing Android app and website for college placement activities Implemented features like coding competitions resume scoring and student job applications Admin panel for staff and mentors to verify and manage results Figma python Flask Django Firebase Flutter LENOVO IN PLANT TRAINING Intern LINK JUN 2023 JULY 2023 Puducherry IN Displayed Professional acumen during the period of In Plant Training in Manufacturing Operations PROJECTS MOVIE VERSE LINK optimal experience Developed user friendly ReactJS app with intuitive interface for Utilized TMDB API to a ess movie database with up to date titles descriptions ratings and images for users Implemented robust search detailed info reviews personalized recommendations for users Sentiment analysis on user reviews using NLP algorithms Naive Bayes Logistic Regression Hosted on Firebase automated deployment via GitHub Actions STREAM SAVER LINK Stream Saver Flask Python web app Download YouTube videos using Pytube library Easy video search and offline saving Simplifies the process of video downloading for users \n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for label, entities in resume_section.items():\n",
    "    if (len(entities) > 0):\n",
    "        print(label.upper())\n",
    "        print(\"-\" * len(label))\n",
    "        for ent in entities:\n",
    "            print(\"* \",ent)\n",
    "        print(\"------\" * 10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find Job Title "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "NER_MODEL_PATH = os.path.join('models', 'jobtitle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def formatJTPayload(dct: dict) -> str:\n",
    "    \"\"\"\n",
    "    Formats the dictionary into a string.\n",
    "    \"\"\"\n",
    "    formatted_str = \"\"\n",
    "    for key, value in dct.items():\n",
    "        formatted_str += f\"{key}: \" + \" \".join(str(i) for i in value) + \"\\n\"\n",
    "    return formatted_str\n",
    "\n",
    "\n",
    "def findJobTitle(payload: str) -> str:\n",
    "    classifier = pickle.load(\n",
    "        open(os.path.join(NER_MODEL_PATH, \"OneVsRestClassifier.pkl\"), \"rb\")\n",
    "    )\n",
    "    vectorizer = pickle.load(\n",
    "        open(os.path.join(NER_MODEL_PATH, \"TfidfVectorizer.pkl\"), \"rb\")\n",
    "    )\n",
    "    label_encoder = pickle.load(\n",
    "        open(os.path.join(NER_MODEL_PATH, \"LabelEncoder.pkl\"), \"rb\")\n",
    "    )\n",
    "\n",
    "    return label_encoder.inverse_transform(classifier.predict(vectorizer.transform([payload])))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "jtPayload = {\n",
    "    \"Projects\": [],\n",
    "    \"Education\": [],\n",
    "    \"Skills\": [],\n",
    "    \"Experience\": [],\n",
    "}\n",
    "\n",
    "for label, entities in resume_section.items():\n",
    "    if (len(entities) > 0):\n",
    "        if label in jtPayload:\n",
    "            jtPayload[label] = entities\n",
    "    else:\n",
    "        if label in jtPayload:\n",
    "            del jtPayload[label]\n",
    "\n",
    "jtPayload = formatJTPayload(jtPayload)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Data Science'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jobTitle = findJobTitle(jtPayload)\n",
    "jobTitle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find O-NET Occupation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "O_NET_MODEL_NAME = \"msmarco-distilbert-base-tas-b\"\n",
    "\n",
    "O_NET_DATASTORE_PATH = os.path.join('dataset/ONET', '2019_Occupations.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained model...\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading pretrained model...\")\n",
    "st_model = SentenceTransformer(O_NET_MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_text(text : str) -> Any:\n",
    "    return st_model.encode([text])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_match_sentence_transformers(input_text : str, data_list : pd.DataFrame) -> tuple:\n",
    "    input_encoding = encode_text(input_text)\n",
    "\n",
    "    best_similarity = -1\n",
    "    best_match = None\n",
    "    best_match_index = None\n",
    "\n",
    "    for i, title in enumerate(data_list):\n",
    "        title_encoding = encode_text(title)\n",
    "        similarity = cosine_similarity([input_encoding], [title_encoding])[0, 0]\n",
    "\n",
    "        if similarity > best_similarity:\n",
    "            best_similarity = similarity\n",
    "            best_match = title\n",
    "            best_match_index = i\n",
    "\n",
    "    return best_match, best_match_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>O*NET-SOC 2019 Code</th>\n",
       "      <th>O*NET-SOC 2019 Title</th>\n",
       "      <th>O*NET-SOC 2019 Description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11-1011.00</td>\n",
       "      <td>Chief Executives</td>\n",
       "      <td>Determine and formulate policies and provide o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11-1011.03</td>\n",
       "      <td>Chief Sustainability Officers</td>\n",
       "      <td>Communicate and coordinate with management, sh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11-1021.00</td>\n",
       "      <td>General and Operations Managers</td>\n",
       "      <td>Plan, direct, or coordinate the operations of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11-1031.00</td>\n",
       "      <td>Legislators</td>\n",
       "      <td>Develop, introduce, or enact laws and statutes...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11-2011.00</td>\n",
       "      <td>Advertising and Promotions Managers</td>\n",
       "      <td>Plan, direct, or coordinate advertising polici...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  O*NET-SOC 2019 Code                 O*NET-SOC 2019 Title  \\\n",
       "0          11-1011.00                     Chief Executives   \n",
       "1          11-1011.03        Chief Sustainability Officers   \n",
       "2          11-1021.00      General and Operations Managers   \n",
       "3          11-1031.00                          Legislators   \n",
       "4          11-2011.00  Advertising and Promotions Managers   \n",
       "\n",
       "                          O*NET-SOC 2019 Description  \n",
       "0  Determine and formulate policies and provide o...  \n",
       "1  Communicate and coordinate with management, sh...  \n",
       "2  Plan, direct, or coordinate the operations of ...  \n",
       "3  Develop, introduce, or enact laws and statutes...  \n",
       "4  Plan, direct, or coordinate advertising polici...  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(O_NET_DATASTORE_PATH)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "codes = df[\"O*NET-SOC 2019 Code\"]\n",
    "titles = df[\"O*NET-SOC 2019 Title\"]\n",
    "description = df[\"O*NET-SOC 2019 Description\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find the best match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding best match...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('Data Scientists', 142)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Finding best match...\")\n",
    "best_match, best_match_index = find_best_match_sentence_transformers(jobTitle, titles)\n",
    "best_match, best_match_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job Title: Data Science\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Occupation: Data Scientists\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Corresponding Code: 15-2051.00\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Description: Develop and implement a set of techniques or analytics applications to transform raw data into meaningful information using data-oriented programming languages and visualization software. Apply data mining, data modeling, natural language processing, and machine learning to extract and analyze information from large structured and unstructured datasets. Visualize, interpret, and report data findings. May create dynamic data reports.\n"
     ]
    }
   ],
   "source": [
    "if best_match is not None:\n",
    "    best_match_code = codes[best_match_index]\n",
    "    print(f\"Job Title: {jobTitle}\")\n",
    "    print(\"-----\" * 20)\n",
    "    print(f\"Occupation: {best_match}\")\n",
    "    print(\"-----\" * 20)\n",
    "    print(f\"Corresponding Code: {best_match_code}\")\n",
    "    print(\"-----\" * 20)\n",
    "    print(f\"Description: {description[best_match_index]}\")\n",
    "else:\n",
    "    print(\"\\nNo similar match found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Skill from Project Description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\tSkills from Project Description\n",
      " --------------------------------------------------------------------\n",
      "1 *  Video\n",
      "2 *  Offline\n",
      "3 *  Analysis\n",
      "4 *  Users\n",
      "5 *  Staff\n",
      "6 *  Service\n",
      "7 *  Authentication\n",
      "8 *  User reviews\n",
      "9 *  Intuitive interface\n",
      "10 *  User\n",
      "11 *  Video search\n",
      "12 *  Website\n",
      "13 *  App\n",
      "14 *  Acumen\n",
      "15 *  Real time data storage\n",
      "16 *  Results\n",
      "17 *  Competitions\n",
      "18 *  Images\n",
      "19 *  College placement activities\n",
      "20 *  Student job applications\n",
      "21 *  Software\n",
      "22 *  Mentors\n",
      "23 *  Ess movie database\n",
      "24 *  Period\n",
      "25 *  Info reviews\n",
      "26 *  Panel\n",
      "27 *  Process\n",
      "28 *  Search\n",
      "29 *  Features\n",
      "30 *  Recommendations\n",
      "31 *  Optimal experience\n",
      "32 *  Js\n",
      "33 *  Date titles descriptions ratings\n"
     ]
    }
   ],
   "source": [
    "def extract_skills(resumeText : str) -> List[str]:\n",
    "    skills = []\n",
    "\n",
    "    pattern = r\"(?i)\\b(?:project|developed|implemented|utilized)\\b[\\s\\w,;:]*\"\n",
    "\n",
    "    project_descriptions = re.findall(pattern, resumeText)\n",
    "\n",
    "    for description in project_descriptions:\n",
    "        \n",
    "        # Tokenize words and tag parts of speech\n",
    "        words = word_tokenize(description)\n",
    "        tagged_words = pos_tag(words)\n",
    "\n",
    "        # Define grammar for noun phrases\n",
    "        grammar = \"NP: {<JJ>*<NN|NNS>+}\"\n",
    "\n",
    "        # Create a chunk parser with the defined grammar\n",
    "        chunk_parser = nltk.RegexpParser(grammar)\n",
    "        chunked_words = chunk_parser.parse(tagged_words)\n",
    "\n",
    "        # Extract the noun phrases (potential skills)\n",
    "        for subtree in chunked_words.subtrees(filter=lambda t: t.label() == \"NP\"):\n",
    "            skill = \" \".join([word for word, tag in subtree.leaves()])\n",
    "            skills.append(skill)\n",
    "\n",
    "    return list(i for i in set(skills) if len(i) > 1)\n",
    "\n",
    "if (len(resume_section['Projects']) > 0):\n",
    "    project_skills = extract_skills(resumeText)\n",
    "    print(\"\\t\\tSkills from Project Description\\n\",'----' * 17)\n",
    "    for idx, i in enumerate(project_skills):\n",
    "        print(f\"{idx + 1} * \",i.capitalize())\n",
    "else:\n",
    "    print(\"No project descriptions found.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mined24",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
