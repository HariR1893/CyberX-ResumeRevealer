{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\anaconda\\envs\\mined24hack\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "e:\\anaconda\\envs\\mined24hack\\Lib\\site-packages\\transformers\\utils\\generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "# Text Extraction\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
    "from pdfminer.converter import TextConverter\n",
    "from pdfminer.layout import LAParams\n",
    "import io\n",
    "from docx2python import docx2python\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# -------------------------------------------------\n",
    "\n",
    "# NER\n",
    "import spacy\n",
    "\n",
    "# -------------------------------------------------\n",
    "\n",
    "# O-NET\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import pandas as pd\n",
    "\n",
    "# --------------------------------------------------\n",
    "\n",
    "# Project Description\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tag import pos_tag\n",
    "\n",
    "import re\n",
    "from typing import List, Any\n",
    "import warnings\n",
    "import os\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\tharu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "try:\n",
    "    nltk.download(\"averaged_perceptron_tagger\")\n",
    "    spacy.load(\"en_core_web_sm\")\n",
    "except:\n",
    "    # !python -m spacy download en_core_web_md\n",
    "    !python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def duration(func: Any) -> Any:\n",
    "    @wraps(func)\n",
    "    def wrapper(*args, **kwargs) -> Any:\n",
    "        start_time = time.time()\n",
    "        result = func(*args, **kwargs)\n",
    "        end_time = time.time()\n",
    "        duration = end_time - start_time\n",
    "        print(f\"Function {func.__name__} took {duration:.4f} seconds\")\n",
    "        return result\n",
    "\n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Extraction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class File2Text:\n",
    "    \"\"\"\n",
    "    A class to extract text content from various types of files such as PDFs, DOCX, and HTML files.\n",
    "\n",
    "    Attributes:\n",
    "        np_of_pages (int): The number of pages in the file.\n",
    "        text (str): The extracted text from the file.\n",
    "        file_path (str): The path to the file.\n",
    "        file_type (str): The type of the file (e.g., \"pdf\", \"docx\", \"html\").\n",
    "\n",
    "    Methods:\n",
    "        __init__(file_path):\n",
    "            Initializes the File2Text object with the given file path.\n",
    "\n",
    "        get_text():\n",
    "            Extracts and returns the text content from the file.\n",
    "\n",
    "        __extractPdfText():\n",
    "            Private method to extract text from a PDF file.\n",
    "\n",
    "        __extractDocx2Text():\n",
    "            Private method to extract text from a DOCX file.\n",
    "\n",
    "        __extractHtml2Text():\n",
    "            Private method to extract text from an HTML file.\n",
    "\n",
    "        __cleanText(text):\n",
    "            Private method to clean and preprocess the extracted text.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, file_path: str) -> None:\n",
    "        \"\"\"\n",
    "        Initializes the File2Text object with the given file path.\n",
    "\n",
    "        Args:\n",
    "            file_path (str): The path to the file.\n",
    "        \"\"\"\n",
    "        self.file_path = file_path\n",
    "        self.file_type = file_path.split(\".\")[-1]\n",
    "\n",
    "    def get_text(self) -> List[str]:\n",
    "        \"\"\"\n",
    "        Extracts and returns the text content from the file.\n",
    "\n",
    "        Returns:\n",
    "            List[str]: A list containing the extracted text.\n",
    "        \"\"\"\n",
    "        __nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "        if self.file_type == \"pdf\":\n",
    "            self.__extractPdfText()\n",
    "        elif self.file_type == \"docx\":\n",
    "            self.__extractDocx2Text()\n",
    "        elif self.file_type == \"html\":\n",
    "            self.__extractHtml2Text()\n",
    "        else:\n",
    "            raise ValueError(\"File type not supported\")\n",
    "        return __nlp(self.__cleanText(self.text)).text\n",
    "\n",
    "    def __extractPdfText(self) -> None:\n",
    "        \"\"\"\n",
    "        Private method to extract text from a PDF file.\n",
    "        \"\"\"\n",
    "        i_f = open(self.file_path, \"rb\")\n",
    "        resMgr = PDFResourceManager()\n",
    "        retData = io.StringIO()\n",
    "        TxtConverter = TextConverter(resMgr, retData, laparams=LAParams())\n",
    "        interpreter = PDFPageInterpreter(resMgr, TxtConverter)\n",
    "        for page in PDFPage.get_pages(i_f):\n",
    "            interpreter.process_page(page)\n",
    "        self.text = retData.getvalue()\n",
    "\n",
    "    def __extractDocx2Text(self) -> None:\n",
    "        \"\"\"\n",
    "        Private method to extract text from a DOCX file.\n",
    "        \"\"\"\n",
    "        self.text = docx2python(self.file_path).text\n",
    "\n",
    "    def __extractHtml2Text(self) -> None:\n",
    "        \"\"\"\n",
    "        Private method to extract text from an HTML file.\n",
    "        \"\"\"\n",
    "        with open(self.file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            html_content = file.read()\n",
    "        soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "        self.text = soup.get_text()\n",
    "\n",
    "    def __cleanText(self, text: str) -> str:\n",
    "        \"\"\"\n",
    "        Private method to clean and preprocess the extracted text.\n",
    "\n",
    "        Args:\n",
    "            text (str): The text to be cleaned.\n",
    "\n",
    "        Returns:\n",
    "            str: The cleaned text.\n",
    "        \"\"\"\n",
    "        # text = text.replace(\"\\n\", \" \")\n",
    "        text = text.replace(\"\\r\", \" \")\n",
    "        # text = text.replace(\"\\t\", \" \").replace(\"\\xa0\", \" \")\n",
    "        text = text.replace(\"\\x0c\", \" \").replace(\"\\x0b\", \" \")\n",
    "        text = text.replace(\"\\x0e\", \" \").replace(\"\\x0f\", \" \").replace(\"\\x1c\", \" \")\n",
    "\n",
    "        text = re.sub(\"http\\S+\\s\", \" \", text)\n",
    "        text = re.sub(\"RT|cc\", \" \", text)\n",
    "        text = re.sub(\"#\\S+\\s\", \" \", text)\n",
    "        text = re.sub(\"@\\S+\", \"  \", text)\n",
    "        text = re.sub(\n",
    "            \"[%s]\" % re.escape(\"\"\"!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\"\"\"), \" \", text\n",
    "        )\n",
    "        text = re.sub(r\"[^\\x00-\\x7f]\", \" \", text)\n",
    "        text = re.sub(\"\\s+\", \" \", text)\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "f2t = File2Text(\"test/easy_resume_level1_a.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Prasham Sheth Data Scientist Phone 1 516 707 1668 Email p d sheth LinkedIn SUMMARY I currently work as a Data Scientist at the SLB Software Technology Innovation Center STIC in Menlo Park California My research interests include Machine Learning and Deep Learning based approaches for solving complex problems in the fields of Computer Vision Prognostic and Health Management and Time Series Analysis Further I am focusing on Hybrid modeling techniques involving Physics Informed Machine Learning EDUCATION Columbia University Master of Science in Data Science GPA 4 08 4 00 Coursework Machine Learning Applied Machine Learning Applied Deep Learning Statistical Inference Modeling Personalization Theory Natural Language Processing Algorithms for Data Science Computer Systems Exploratory Data Analysis and Visualization Nirma University Bachelor of Technology in Computer Engineering GPA 9 50 10 Rank 2 900 Coursework Machine Learning Deep Learning Artificial Intelligence Linear Algebra Algorithms WORK EXPERIENCE SLB Software Technology Innovation Center STIC Data Scientist Physics Informed Machine Learning and Time Series Analysis Developed Physics Informed Machine Learning based Hybrid Framework to create an advisory system that identifies the Menlo Park CA Feb 2021 Present New York NY Dec 2020 Ahmedabad India May 2019 regions with risky Stick Slip conditions and outputs an optimal operating window for drilling the future stands Devised Hybrid framework using Physics Informed Machine Learning for digitally generating LWD logs Gamma Ray logs in real time to increase efficiency and robustness of log collection process Working towards implementation of Physics Inspired Machine Learning based toolbox for Time Series data Computer Vision Researched and implemented various Computer Vision based use cases to improve Health Safety and Environment Evaluated 5 state of the at solutions for Object Detection Object Tracking and Pose Estimation Fine tuned different object detection models and integrated them into the pose estimation pipelines for increased performance for field datasets Media Center of Art and History at Columbia University Graduate Research Assistant Engineered way to automate process of slide analysis for collection of slides deploying image processing and ML DL New York NY Mar 2020 Dec 2020 techniques to detect originality of photographic images in 35mm slides collection Formulated pipeline to replicate entire manual process by initially filtering images based on citation used in slides followed by identifying presence of Halftone patterns Precision 95 Recall 82 Unilever Data Science Analyst Developed and deployed an application to streamline the feature extraction and data engineering process for Process Englewood Cliffs NJ Mar 2020 Dec 2020 Analytics Engine Supported development of Process Analytics Engine to get the insights interactively using the data collected from production unit 1 Swiss Reinsurance America Corporation Data Scientist Intern Designed and developed Smart Underwriting Framework to generate scores for each submission based on propensity to Armonk NY May 2020 Aug 2020 bind Prioritizing processing for submissions based on scores improved Cumulative Gains by 20 Analyzed Drift in Data using KL Divergence and Logistic Regression based models constructed technique to retrain model dynamically in production setting to ensure Domain Adaptability Researched various papers articles and datasets available for Individual Health Forecasting Review task resulted in document summarizing 89 research papers involving DL based approaches for Electronic Health Records Samsung R D Institute Research Intern Researched various On Device AI solutions as part of AI core team and contributed to enhancing quality of services provided by Samsung for its mobile devices Produced ML DL models by utilizing Scikit learn TensorFlow TF Lite frameworks Noida India Jan 2019 May 2019 Devised techniques for Facial Anti Spoofing System leveraging various ML DL methods in Python and deployed it as an Android Application Analyzed different On Device AI solutions for health and multimedia services on low end Samsung smartphones with 1GB of RAM to ensure robustness of solutions PUBLICATIONS Journal Publications 1 Param Popat Prasham Sheth and Swati Jain Animal Object Identification Using Deep Learning on Raspberry Pi In Information and Communication Technology for Intelligent Systems Proceedings of ICTIS 2018 Volume 1 pp 319 327 Springer Singapore 2019 2 Prasham Sheth Priyank Thakkar and Praxal Patel Optimal Location Prediction for Emergency Stations Using Machine Learning International Journal of Operational Research 2022 3 Prasham Sheth Sai Shravani Sistla Indranil Roychoudhury Mengdi Gao Crispin Chatar Jose Celaya and Priya Mishra Real Time Gamma Ray Log Generation from Drilling Parameters of Offset Wells Using Physics Informed Machine Learning SPE Journal 2023 1 11 Conference Publications 1 Prasham Sheth Indranil Roychoudhury Crispin Chatar and Jos Celaya A Hybrid Physics Based and Machine Learning Approach for Stick Slip Prediction In IADC SPE International Drilling Conference and Exhibition OnePetro 2022 2 Prasham Sheth Sai Shravani Sistla Indranil Roychoudhury Mengdi Gao Crispin Chatar Jose Celaya and Priya Mishra Real Time Digital Log Generation from Drilling Parameters of Offset Wells Using Physics Informed Machine Learning In SPE IADC International Drilling Conference and Exhibition OnePetro 2023 ACADEMIC PROJECTS Energy Efficient AI on Edge Devices Master Thesis Project Developed techniques for compressing Deep Learning Models for faster inference on edge devices and reduced carbon Sep 2020 Dec 2020 footprint in association with GE Research Researched Post training quantization Quantization Aware Training QAT and model Pruning techniques for model compressing Designed low latency compressed models with minimal a uracy drop for formulated models for multiple datasets Hybrid approach combining Content and Model based Techniques for Recommendation Sep 2019 Dec 2019 Created Restaurant Recommendation System based on Yelp dataset 2019 6 685 900 reviews 192 609 businesses 200 000 pictures ten metropolitan areas over 1 2 million business attributes hours parking availability and ambiance Developed Recommendation System utilizing various techniques Alternating Least Square based Matrix Factorization Factorization Machines Content based Recommendation capturing information from Images Text based reviews 2 A elerated performance of Recommendation Engine by engineering way to combine results from various approaches to cultivate strengths of each model and get more generalized recommendations Optimal Location Prediction for Emergency Stations Jul 2018 Jan 2019 Identified and engineered various influencing parameters namely location attributes population density traffic navigation frequency of ambulance calls and weather conditions Formulated various ML DL Models to identify best suitable for Travel Time Estimation between different locations of city Travel Time estimated from XGBoost is used to drive K Medoids for predicting optimal locations Conceptualized approach demonstrated promising test results Decreased turnaround time along with reduced utility of resources for Staten Island average time reduced by 6 seconds utilizing 14 Fire Stations in comparison to 19 actual ones End to End Sentence Level Lipreading Jan 2018 Apr 2018 Designed model for performing end to end sentence level lip reading rather than approaches of detecting individual words by simultaneously recognizing spatiotemporal visual features and sequence model Improved performance by approximately 10 over the LSTM Baseline Structured method to process frames captured at 25 fps from GRID Corpus leveraging combination of CNN and Bidirectional Gated Recurrent Units Bi GRUs to enhance performance of end to end sentence formation SKILLS Programming Languages Python SQL R Java C C Tools and Technologies Scikit Learn NumPy Pandas Statsmodels PyTorch OpenCV Scipy Google BigQuery Oracle DS MongoDB Google Cloud Platform GitHub LaTeX 3 '"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resumeText = f2t.get_text()\n",
    "resumeText"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resume Section\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_project_section(text: str) -> Any:\n",
    "    pattern = r\"(?i)\\b(?:project|developed|implemented|utilized)\\b[\\s\\w,;:]*\"\n",
    "    return re.findall(pattern, resumeText)\n",
    "\n",
    "\n",
    "def extract_education_section(text: str) -> Any:\n",
    "    pattern = r\"(?i)education(?:[\\s\\w,;:()-]*graduated[\\s\\w,;:()-]*:(.*?)(?=(?:[\\s\\w,;:()-]*\\b\\d{4}\\b)|(?:[\\s\\w,;:()-]*$)))\"\n",
    "    education_sections = re.finditer(pattern, resumeText)\n",
    "    return str(education_sections.group())\n",
    "\n",
    "\n",
    "def extract_skills(resumeText):\n",
    "    skills = []\n",
    "\n",
    "    project_descriptions = extract_project_section(resumeText)\n",
    "\n",
    "    for description in project_descriptions:\n",
    "        # Tokenize words and tag parts of speech\n",
    "        words = word_tokenize(description)\n",
    "        tagged_words = pos_tag(words)\n",
    "\n",
    "        # Define grammar for noun phrases\n",
    "        grammar = \"NP: {<JJ>*<NN|NNS>+}\"\n",
    "\n",
    "        # Create a chunk parser with the defined grammar\n",
    "        chunk_parser = nltk.RegexpParser(grammar)\n",
    "        chunked_words = chunk_parser.parse(tagged_words)\n",
    "\n",
    "        # Extract the noun phrases (potential skills)\n",
    "        for subtree in chunked_words.subtrees(filter=lambda t: t.label() == \"NP\"):\n",
    "            skill = \" \".join([word for word, tag in subtree.leaves()])\n",
    "            skills.append(skill)\n",
    "\n",
    "    return list(set(skills))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "CUSTOM_NER_MODEL_PATH = os.path.join(\"models/ner/JdModel/output\", \"model-best\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "resume_ner_model = spacy.load(CUSTOM_NER_MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'transformer': [],\n",
       " 'ner': ['CERTIFICATION', 'DEGREE', 'EXPERIENCE', 'JOBPOST', 'SKILLS']}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resume_ner_model.pipe_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "resume_section = {\n",
    "    label.capitalize(): [] for label in resume_ner_model.pipe_labels[\"ner\"]\n",
    "}\n",
    "resume_section[\"Projects\"] = (\n",
    "    extract_project_section(resumeText)\n",
    "    if len(extract_project_section(resumeText)) > 0\n",
    "    else []\n",
    ")\n",
    "resume_section[\"Education\"] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = resume_ner_model(resumeText)\n",
    "for ent in doc.ents:\n",
    "    resume_section[ent.label_.capitalize()].append(ent.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(extract_skills(resume_section[\"Projects\"])) == 0:\n",
    "    resume_section[\"Skills\"] = extract_skills(resume_section[\"Projects\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(resume_section[\"Education\"]) != 0:\n",
    "    resume_section[\"Education\"] = extract_education_section(resumeText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Certification': [],\n",
       " 'Degree': ['Bachelor of Technology in Computer Engineering GPA 9 50 10'],\n",
       " 'Experience': [],\n",
       " 'Jobpost': [],\n",
       " 'Skills': ['Deep Learning',\n",
       "  'Deep Learning',\n",
       "  'TensorFlow',\n",
       "  'CNN',\n",
       "  'Python',\n",
       "  'SQL',\n",
       "  'R',\n",
       "  'Java C C Tools',\n",
       "  'Scikit',\n",
       "  'NumPy',\n",
       "  'PyTorch',\n",
       "  'OpenCV',\n",
       "  'Scipy',\n",
       "  'BigQuery',\n",
       "  'Oracle',\n",
       "  'MongoDB',\n",
       "  'LaTeX'],\n",
       " 'Projects': ['Developed Physics Informed Machine Learning based Hybrid Framework to create an advisory system that identifies the Menlo Park CA Feb 2021 Present New York NY Dec 2020 Ahmedabad India May 2019 regions with risky Stick Slip conditions and outputs an optimal operating window for drilling the future stands Devised Hybrid framework using Physics Informed Machine Learning for digitally generating LWD logs Gamma Ray logs in real time to increase efficiency and robustness of log collection process Working towards implementation of Physics Inspired Machine Learning based toolbox for Time Series data Computer Vision Researched and implemented various Computer Vision based use cases to improve Health Safety and Environment Evaluated 5 state of the at solutions for Object Detection Object Tracking and Pose Estimation Fine tuned different object detection models and integrated them into the pose estimation pipelines for increased performance for field datasets Media Center of Art and History at Columbia University Graduate Research Assistant Engineered way to automate process of slide analysis for collection of slides deploying image processing and ML DL New York NY Mar 2020 Dec 2020 techniques to detect originality of photographic images in 35mm slides collection Formulated pipeline to replicate entire manual process by initially filtering images based on citation used in slides followed by identifying presence of Halftone patterns Precision 95 Recall 82 Unilever Data Science Analyst Developed and deployed an application to streamline the feature extraction and data engineering process for Process Englewood Cliffs NJ Mar 2020 Dec 2020 Analytics Engine Supported development of Process Analytics Engine to get the insights interactively using the data collected from production unit 1 Swiss Reinsurance America Corporation Data Scientist Intern Designed and developed Smart Underwriting Framework to generate scores for each submission based on propensity to Armonk NY May 2020 Aug 2020 bind Prioritizing processing for submissions based on scores improved Cumulative Gains by 20 Analyzed Drift in Data using KL Divergence and Logistic Regression based models constructed technique to retrain model dynamically in production setting to ensure Domain Adaptability Researched various papers articles and datasets available for Individual Health Forecasting Review task resulted in document summarizing 89 research papers involving DL based approaches for Electronic Health Records Samsung R D Institute Research Intern Researched various On Device AI solutions as part of AI core team and contributed to enhancing quality of services provided by Samsung for its mobile devices Produced ML DL models by utilizing Scikit learn TensorFlow TF Lite frameworks Noida India Jan 2019 May 2019 Devised techniques for Facial Anti Spoofing System leveraging various ML DL methods in Python and deployed it as an Android Application Analyzed different On Device AI solutions for health and multimedia services on low end Samsung smartphones with 1GB of RAM to ensure robustness of solutions PUBLICATIONS Journal Publications 1 Param Popat Prasham Sheth and Swati Jain Animal Object Identification Using Deep Learning on Raspberry Pi In Information and Communication Technology for Intelligent Systems Proceedings of ICTIS 2018 Volume 1 pp 319 327 Springer Singapore 2019 2 Prasham Sheth Priyank Thakkar and Praxal Patel Optimal Location Prediction for Emergency Stations Using Machine Learning International Journal of Operational Research 2022 3 Prasham Sheth Sai Shravani Sistla Indranil Roychoudhury Mengdi Gao Crispin Chatar Jose Celaya and Priya Mishra Real Time Gamma Ray Log Generation from Drilling Parameters of Offset Wells Using Physics Informed Machine Learning SPE Journal 2023 1 11 Conference Publications 1 Prasham Sheth Indranil Roychoudhury Crispin Chatar and Jos Celaya A Hybrid Physics Based and Machine Learning Approach for Stick Slip Prediction In IADC SPE International Drilling Conference and Exhibition OnePetro 2022 2 Prasham Sheth Sai Shravani Sistla Indranil Roychoudhury Mengdi Gao Crispin Chatar Jose Celaya and Priya Mishra Real Time Digital Log Generation from Drilling Parameters of Offset Wells Using Physics Informed Machine Learning In SPE IADC International Drilling Conference and Exhibition OnePetro 2023 ACADEMIC PROJECTS Energy Efficient AI on Edge Devices Master Thesis Project Developed techniques for compressing Deep Learning Models for faster inference on edge devices and reduced carbon Sep 2020 Dec 2020 footprint in association with GE Research Researched Post training quantization Quantization Aware Training QAT and model Pruning techniques for model compressing Designed low latency compressed models with minimal a uracy drop for formulated models for multiple datasets Hybrid approach combining Content and Model based Techniques for Recommendation Sep 2019 Dec 2019 Created Restaurant Recommendation System based on Yelp dataset 2019 6 685 900 reviews 192 609 businesses 200 000 pictures ten metropolitan areas over 1 2 million business attributes hours parking availability and ambiance Developed Recommendation System utilizing various techniques Alternating Least Square based Matrix Factorization Factorization Machines Content based Recommendation capturing information from Images Text based reviews 2 A elerated performance of Recommendation Engine by engineering way to combine results from various approaches to cultivate strengths of each model and get more generalized recommendations Optimal Location Prediction for Emergency Stations Jul 2018 Jan 2019 Identified and engineered various influencing parameters namely location attributes population density traffic navigation frequency of ambulance calls and weather conditions Formulated various ML DL Models to identify best suitable for Travel Time Estimation between different locations of city Travel Time estimated from XGBoost is used to drive K Medoids for predicting optimal locations Conceptualized approach demonstrated promising test results Decreased turnaround time along with reduced utility of resources for Staten Island average time reduced by 6 seconds utilizing 14 Fire Stations in comparison to 19 actual ones End to End Sentence Level Lipreading Jan 2018 Apr 2018 Designed model for performing end to end sentence level lip reading rather than approaches of detecting individual words by simultaneously recognizing spatiotemporal visual features and sequence model Improved performance by approximately 10 over the LSTM Baseline Structured method to process frames captured at 25 fps from GRID Corpus leveraging combination of CNN and Bidirectional Gated Recurrent Units Bi GRUs to enhance performance of end to end sentence formation SKILLS Programming Languages Python SQL R Java C C Tools and Technologies Scikit Learn NumPy Pandas Statsmodels PyTorch OpenCV Scipy Google BigQuery Oracle DS MongoDB Google Cloud Platform GitHub LaTeX 3 '],\n",
       " 'Education': []}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resume_section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEGREE\n",
      "------\n",
      "*  Bachelor of Technology in Computer Engineering GPA 9 50 10\n",
      "------------------------------------------------------------\n",
      "SKILLS\n",
      "------\n",
      "*  Deep Learning\n",
      "*  Deep Learning\n",
      "*  TensorFlow\n",
      "*  CNN\n",
      "*  Python\n",
      "*  SQL\n",
      "*  R\n",
      "*  Java C C Tools\n",
      "*  Scikit\n",
      "*  NumPy\n",
      "*  PyTorch\n",
      "*  OpenCV\n",
      "*  Scipy\n",
      "*  BigQuery\n",
      "*  Oracle\n",
      "*  MongoDB\n",
      "*  LaTeX\n",
      "------------------------------------------------------------\n",
      "PROJECTS\n",
      "--------\n",
      "*  Developed Physics Informed Machine Learning based Hybrid Framework to create an advisory system that identifies the Menlo Park CA Feb 2021 Present New York NY Dec 2020 Ahmedabad India May 2019 regions with risky Stick Slip conditions and outputs an optimal operating window for drilling the future stands Devised Hybrid framework using Physics Informed Machine Learning for digitally generating LWD logs Gamma Ray logs in real time to increase efficiency and robustness of log collection process Working towards implementation of Physics Inspired Machine Learning based toolbox for Time Series data Computer Vision Researched and implemented various Computer Vision based use cases to improve Health Safety and Environment Evaluated 5 state of the at solutions for Object Detection Object Tracking and Pose Estimation Fine tuned different object detection models and integrated them into the pose estimation pipelines for increased performance for field datasets Media Center of Art and History at Columbia University Graduate Research Assistant Engineered way to automate process of slide analysis for collection of slides deploying image processing and ML DL New York NY Mar 2020 Dec 2020 techniques to detect originality of photographic images in 35mm slides collection Formulated pipeline to replicate entire manual process by initially filtering images based on citation used in slides followed by identifying presence of Halftone patterns Precision 95 Recall 82 Unilever Data Science Analyst Developed and deployed an application to streamline the feature extraction and data engineering process for Process Englewood Cliffs NJ Mar 2020 Dec 2020 Analytics Engine Supported development of Process Analytics Engine to get the insights interactively using the data collected from production unit 1 Swiss Reinsurance America Corporation Data Scientist Intern Designed and developed Smart Underwriting Framework to generate scores for each submission based on propensity to Armonk NY May 2020 Aug 2020 bind Prioritizing processing for submissions based on scores improved Cumulative Gains by 20 Analyzed Drift in Data using KL Divergence and Logistic Regression based models constructed technique to retrain model dynamically in production setting to ensure Domain Adaptability Researched various papers articles and datasets available for Individual Health Forecasting Review task resulted in document summarizing 89 research papers involving DL based approaches for Electronic Health Records Samsung R D Institute Research Intern Researched various On Device AI solutions as part of AI core team and contributed to enhancing quality of services provided by Samsung for its mobile devices Produced ML DL models by utilizing Scikit learn TensorFlow TF Lite frameworks Noida India Jan 2019 May 2019 Devised techniques for Facial Anti Spoofing System leveraging various ML DL methods in Python and deployed it as an Android Application Analyzed different On Device AI solutions for health and multimedia services on low end Samsung smartphones with 1GB of RAM to ensure robustness of solutions PUBLICATIONS Journal Publications 1 Param Popat Prasham Sheth and Swati Jain Animal Object Identification Using Deep Learning on Raspberry Pi In Information and Communication Technology for Intelligent Systems Proceedings of ICTIS 2018 Volume 1 pp 319 327 Springer Singapore 2019 2 Prasham Sheth Priyank Thakkar and Praxal Patel Optimal Location Prediction for Emergency Stations Using Machine Learning International Journal of Operational Research 2022 3 Prasham Sheth Sai Shravani Sistla Indranil Roychoudhury Mengdi Gao Crispin Chatar Jose Celaya and Priya Mishra Real Time Gamma Ray Log Generation from Drilling Parameters of Offset Wells Using Physics Informed Machine Learning SPE Journal 2023 1 11 Conference Publications 1 Prasham Sheth Indranil Roychoudhury Crispin Chatar and Jos Celaya A Hybrid Physics Based and Machine Learning Approach for Stick Slip Prediction In IADC SPE International Drilling Conference and Exhibition OnePetro 2022 2 Prasham Sheth Sai Shravani Sistla Indranil Roychoudhury Mengdi Gao Crispin Chatar Jose Celaya and Priya Mishra Real Time Digital Log Generation from Drilling Parameters of Offset Wells Using Physics Informed Machine Learning In SPE IADC International Drilling Conference and Exhibition OnePetro 2023 ACADEMIC PROJECTS Energy Efficient AI on Edge Devices Master Thesis Project Developed techniques for compressing Deep Learning Models for faster inference on edge devices and reduced carbon Sep 2020 Dec 2020 footprint in association with GE Research Researched Post training quantization Quantization Aware Training QAT and model Pruning techniques for model compressing Designed low latency compressed models with minimal a uracy drop for formulated models for multiple datasets Hybrid approach combining Content and Model based Techniques for Recommendation Sep 2019 Dec 2019 Created Restaurant Recommendation System based on Yelp dataset 2019 6 685 900 reviews 192 609 businesses 200 000 pictures ten metropolitan areas over 1 2 million business attributes hours parking availability and ambiance Developed Recommendation System utilizing various techniques Alternating Least Square based Matrix Factorization Factorization Machines Content based Recommendation capturing information from Images Text based reviews 2 A elerated performance of Recommendation Engine by engineering way to combine results from various approaches to cultivate strengths of each model and get more generalized recommendations Optimal Location Prediction for Emergency Stations Jul 2018 Jan 2019 Identified and engineered various influencing parameters namely location attributes population density traffic navigation frequency of ambulance calls and weather conditions Formulated various ML DL Models to identify best suitable for Travel Time Estimation between different locations of city Travel Time estimated from XGBoost is used to drive K Medoids for predicting optimal locations Conceptualized approach demonstrated promising test results Decreased turnaround time along with reduced utility of resources for Staten Island average time reduced by 6 seconds utilizing 14 Fire Stations in comparison to 19 actual ones End to End Sentence Level Lipreading Jan 2018 Apr 2018 Designed model for performing end to end sentence level lip reading rather than approaches of detecting individual words by simultaneously recognizing spatiotemporal visual features and sequence model Improved performance by approximately 10 over the LSTM Baseline Structured method to process frames captured at 25 fps from GRID Corpus leveraging combination of CNN and Bidirectional Gated Recurrent Units Bi GRUs to enhance performance of end to end sentence formation SKILLS Programming Languages Python SQL R Java C C Tools and Technologies Scikit Learn NumPy Pandas Statsmodels PyTorch OpenCV Scipy Google BigQuery Oracle DS MongoDB Google Cloud Platform GitHub LaTeX 3 \n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for label, entities in resume_section.items():\n",
    "    if len(entities) > 0:\n",
    "        print(label.upper())\n",
    "        print(\"-\" * len(label))\n",
    "        for ent in entities:\n",
    "            print(\"* \", ent)\n",
    "        print(\"------\" * 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find Job Title\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "NER_MODEL_PATH = os.path.join(\"models\", \"jobtitle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def formatJTPayload(dct: dict) -> str:\n",
    "    \"\"\"\n",
    "    Formats the dictionary into a string.\n",
    "    \"\"\"\n",
    "    formatted_str = \"\"\n",
    "    for key, value in dct.items():\n",
    "        formatted_str += f\"{key}: \" + \" \".join(str(i) for i in value) + \"\\n\"\n",
    "    return formatted_str\n",
    "\n",
    "\n",
    "def findJobTitle(payload: str) -> str:\n",
    "    classifier = pickle.load(\n",
    "        open(os.path.join(NER_MODEL_PATH, \"OneVsRestClassifier.pkl\"), \"rb\")\n",
    "    )\n",
    "    vectorizer = pickle.load(\n",
    "        open(os.path.join(NER_MODEL_PATH, \"TfidfVectorizer.pkl\"), \"rb\")\n",
    "    )\n",
    "    label_encoder = pickle.load(\n",
    "        open(os.path.join(NER_MODEL_PATH, \"LabelEncoder.pkl\"), \"rb\")\n",
    "    )\n",
    "\n",
    "    return label_encoder.inverse_transform(\n",
    "        classifier.predict(vectorizer.transform([payload]))\n",
    "    )[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "jtPayload = {\n",
    "    \"Projects\": [],\n",
    "    \"Education\": [],\n",
    "    \"Skills\": [],\n",
    "    \"Experience\": [],\n",
    "}\n",
    "\n",
    "for label, entities in resume_section.items():\n",
    "    if len(entities) > 0:\n",
    "        if label in jtPayload:\n",
    "            jtPayload[label] = entities\n",
    "    else:\n",
    "        if label in jtPayload:\n",
    "            del jtPayload[label]\n",
    "\n",
    "jtPayload = formatJTPayload(jtPayload)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Data Science'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jobTitle = findJobTitle(jtPayload)\n",
    "jobTitle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find O-NET Occupation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "O_NET_MODEL_NAME = \"msmarco-distilbert-base-tas-b\"\n",
    "\n",
    "O_NET_DATASTORE_PATH = os.path.join(\"dataset/ONET\", \"2019_Occupations.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained model...\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading pretrained model...\")\n",
    "st_model = SentenceTransformer(O_NET_MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_text(text: str) -> Any:\n",
    "    return st_model.encode([text])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_match_sentence_transformers(\n",
    "    input_text: str, data_list: pd.DataFrame\n",
    ") -> tuple:\n",
    "    input_encoding = encode_text(input_text)\n",
    "\n",
    "    best_similarity = -1\n",
    "    best_match = None\n",
    "    best_match_index = None\n",
    "\n",
    "    for i, title in enumerate(data_list):\n",
    "        title_encoding = encode_text(title)\n",
    "        similarity = cosine_similarity([input_encoding], [title_encoding])[0, 0]\n",
    "\n",
    "        if similarity > best_similarity:\n",
    "            best_similarity = similarity\n",
    "            best_match = title\n",
    "            best_match_index = i\n",
    "\n",
    "    return best_match, best_match_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>O*NET-SOC 2019 Code</th>\n",
       "      <th>O*NET-SOC 2019 Title</th>\n",
       "      <th>O*NET-SOC 2019 Description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11-1011.00</td>\n",
       "      <td>Chief Executives</td>\n",
       "      <td>Determine and formulate policies and provide o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11-1011.03</td>\n",
       "      <td>Chief Sustainability Officers</td>\n",
       "      <td>Communicate and coordinate with management, sh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11-1021.00</td>\n",
       "      <td>General and Operations Managers</td>\n",
       "      <td>Plan, direct, or coordinate the operations of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11-1031.00</td>\n",
       "      <td>Legislators</td>\n",
       "      <td>Develop, introduce, or enact laws and statutes...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11-2011.00</td>\n",
       "      <td>Advertising and Promotions Managers</td>\n",
       "      <td>Plan, direct, or coordinate advertising polici...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  O*NET-SOC 2019 Code                 O*NET-SOC 2019 Title  \\\n",
       "0          11-1011.00                     Chief Executives   \n",
       "1          11-1011.03        Chief Sustainability Officers   \n",
       "2          11-1021.00      General and Operations Managers   \n",
       "3          11-1031.00                          Legislators   \n",
       "4          11-2011.00  Advertising and Promotions Managers   \n",
       "\n",
       "                          O*NET-SOC 2019 Description  \n",
       "0  Determine and formulate policies and provide o...  \n",
       "1  Communicate and coordinate with management, sh...  \n",
       "2  Plan, direct, or coordinate the operations of ...  \n",
       "3  Develop, introduce, or enact laws and statutes...  \n",
       "4  Plan, direct, or coordinate advertising polici...  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(O_NET_DATASTORE_PATH)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "codes = df[\"O*NET-SOC 2019 Code\"]\n",
    "titles = df[\"O*NET-SOC 2019 Title\"]\n",
    "description = df[\"O*NET-SOC 2019 Description\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find the best match\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding best match...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('Data Scientists', 142)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Finding best match...\")\n",
    "best_match, best_match_index = find_best_match_sentence_transformers(jobTitle, titles)\n",
    "best_match, best_match_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job Title: Data Science\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Occupation: Data Scientists\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Corresponding Code: 15-2051.00\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Description: Develop and implement a set of techniques or analytics applications to transform raw data into meaningful information using data-oriented programming languages and visualization software. Apply data mining, data modeling, natural language processing, and machine learning to extract and analyze information from large structured and unstructured datasets. Visualize, interpret, and report data findings. May create dynamic data reports.\n"
     ]
    }
   ],
   "source": [
    "if best_match is not None:\n",
    "    best_match_code = codes[best_match_index]\n",
    "    print(f\"Job Title: {jobTitle}\")\n",
    "    print(\"-----\" * 20)\n",
    "    print(f\"Occupation: {best_match}\")\n",
    "    print(\"-----\" * 20)\n",
    "    print(f\"Corresponding Code: {best_match_code}\")\n",
    "    print(\"-----\" * 20)\n",
    "    print(f\"Description: {description[best_match_index]}\")\n",
    "else:\n",
    "    print(\"\\nNo similar match found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Skill from Project Description\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\tSkills from Project Description\n",
      " --------------------------------------------------------------------\n",
      "1 *  Multimedia services\n",
      "2 *  End\n",
      "3 *  Sentence formation\n",
      "4 *  Log collection process\n",
      "5 *  Optimal operating window\n",
      "6 *  State\n",
      "7 *  Health\n",
      "8 *  Smartphones\n",
      "9 *  Uracy drop\n",
      "10 *  Production setting\n",
      "11 *  Towards implementation\n",
      "12 *  Attributes\n",
      "13 *  Datasets\n",
      "14 *  Image processing\n",
      "15 *  Data\n",
      "16 *  Combination\n",
      "17 *  Pictures\n",
      "18 *  Logs\n",
      "19 *  Low end\n",
      "20 *  Conditions\n",
      "21 *  Footprint\n",
      "22 *  Edge devices\n",
      "23 *  Field datasets\n",
      "24 *  Scores\n",
      "25 *  Future\n",
      "26 *  Technique\n",
      "27 *  Presence\n",
      "28 *  Sequence model\n",
      "29 *  Toolbox\n",
      "30 *  Association\n",
      "31 *  Core team\n",
      "32 *  Reduced utility\n",
      "33 *  Insights\n",
      "34 *  Sentence level lip\n",
      "35 *  Turnaround time\n",
      "36 *  Promising test results\n",
      "37 *  Comparison\n",
      "38 *  Submission\n",
      "39 *  Training quantization\n",
      "40 *  Fps\n",
      "41 *  Engineering way\n",
      "42 *  Way\n",
      "43 *  Document\n",
      "44 *  Information\n",
      "45 *  Processing\n",
      "46 *  Parameters\n",
      "47 *  Approaches\n",
      "48 *  Images\n",
      "49 *  Formulated models\n",
      "50 *  Propensity\n",
      "51 *  Task\n",
      "52 *  Robustness\n",
      "53 *  Density traffic navigation frequency\n",
      "54 *  City\n",
      "55 *  Pose estimation pipelines\n",
      "56 *  Application\n",
      "57 *  Data engineering process\n",
      "58 *  Individual words\n",
      "59 *  Various papers articles\n",
      "60 *  Actual ones end\n",
      "61 *  Techniques\n",
      "62 *  Reviews\n",
      "63 *  Framework\n",
      "64 *  Regions\n",
      "65 *  Different object detection models\n",
      "66 *  Businesses\n",
      "67 *  Services\n",
      "68 *  Strengths\n",
      "69 *  Inference\n",
      "70 *  Results\n",
      "71 *  Citation\n",
      "72 *  Generalized recommendations\n",
      "73 *  Slides collection\n",
      "74 *  Slides\n",
      "75 *  Seconds\n",
      "76 *  Research papers\n",
      "77 *  Mobile devices\n",
      "78 *  Performance\n",
      "79 *  Process\n",
      "80 *  Different locations\n",
      "81 *  Ambiance\n",
      "82 *  Xgboost\n",
      "83 *  Submissions\n",
      "84 *  Low latency\n",
      "85 *  Real time\n",
      "86 *  Various techniques\n",
      "87 *  Increased performance\n",
      "88 *  Optimal locations\n",
      "89 *  Production unit\n",
      "90 *  Part\n",
      "91 *  Resources\n",
      "92 *  Various approaches\n",
      "93 *  Development\n",
      "94 *  Slide analysis\n",
      "95 *  Methods\n",
      "96 *  Originality\n",
      "97 *  Pp\n",
      "98 *  Elerated performance\n",
      "99 *  Volume\n",
      "100 *  Physics\n",
      "101 *  Parameters\n",
      "102 *  Multiple datasets\n",
      "103 *  Average time\n",
      "104 *  Cumulative gains\n",
      "105 *  Approach\n",
      "106 *  Model\n",
      "107 *  Method\n",
      "108 *  Reduced carbon\n",
      "109 *  Spatiotemporal visual features\n",
      "110 *  Collection\n",
      "111 *  Availability\n",
      "112 *  Photographic images\n",
      "113 *  Solutions\n",
      "114 *  Weather conditions\n",
      "115 *  Ten metropolitan areas\n",
      "116 *  Models\n",
      "117 *  Learn\n",
      "118 *  Use cases\n",
      "119 *  Advisory system\n",
      "120 *  Entire manual process\n",
      "121 *  Efficiency\n",
      "122 *  Pipeline\n",
      "123 *  Ambulance calls\n",
      "124 *  Feature extraction\n",
      "125 *  Quality\n",
      "126 *  Business attributes hours\n"
     ]
    }
   ],
   "source": [
    "def extract_skills(resumeText: str) -> List[str]:\n",
    "    skills = []\n",
    "\n",
    "    pattern = r\"(?i)\\b(?:project|developed|implemented|utilized)\\b[\\s\\w,;:]*\"\n",
    "\n",
    "    project_descriptions = re.findall(pattern, resumeText)\n",
    "\n",
    "    for description in project_descriptions:\n",
    "\n",
    "        # Tokenize words and tag parts of speech\n",
    "        words = word_tokenize(description)\n",
    "        tagged_words = pos_tag(words)\n",
    "\n",
    "        # Define grammar for noun phrases\n",
    "        grammar = \"NP: {<JJ>*<NN|NNS>+}\"\n",
    "\n",
    "        # Create a chunk parser with the defined grammar\n",
    "        chunk_parser = nltk.RegexpParser(grammar)\n",
    "        chunked_words = chunk_parser.parse(tagged_words)\n",
    "\n",
    "        # Extract the noun phrases (potential skills)\n",
    "        for subtree in chunked_words.subtrees(filter=lambda t: t.label() == \"NP\"):\n",
    "            skill = \" \".join([word for word, tag in subtree.leaves()])\n",
    "            skills.append(skill)\n",
    "\n",
    "    return list(i for i in set(skills) if len(i) > 1)\n",
    "\n",
    "\n",
    "if len(resume_section[\"Projects\"]) > 0:\n",
    "    project_skills = extract_skills(resumeText)\n",
    "    print(\"\\t\\tSkills from Project Description\\n\", \"----\" * 17)\n",
    "    for idx, i in enumerate(project_skills):\n",
    "        print(f\"{idx + 1} * \", i.capitalize())\n",
    "else:\n",
    "    print(\"No project descriptions found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mined24",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
