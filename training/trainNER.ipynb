{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import pickle\n",
    "import random\n",
    "import json\n",
    "import time\n",
    "from functools import wraps\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.4\n"
     ]
    }
   ],
   "source": [
    "print(spacy.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCH = 10\n",
    "DROPOUT = 0.2\n",
    "\n",
    "CUSTOM_NER_MODEL_PATH = \"../models/ner/resume_ner_model\"\n",
    "NER_DATASET_PATH = \"../dataset/ner\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def duration(func):\n",
    "    @wraps(func)\n",
    "    def wrapper(*args, **kwargs):\n",
    "        start_time = time.time()\n",
    "        result = func(*args, **kwargs)\n",
    "        end_time = time.time()\n",
    "        duration = end_time - start_time\n",
    "        print(f\"Function {func.__name__} took {duration:.4f} seconds\")\n",
    "        return result\n",
    "\n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data1 = pickle.load(open(os.path.join(NER_DATASET_PATH, \"train_data.pkl\"), \"rb\"))\n",
    "\n",
    "with open(os.path.join(NER_DATASET_PATH, \"train_data.json\"), \"r\") as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data consist of 400 manually labelled resume's.\n"
     ]
    }
   ],
   "source": [
    "train_data = train_data1\n",
    "train_data.extend(data)\n",
    "print(f\"Training data consist of {len(train_data)} manually labelled resume's.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': 'Govardhana K Senior Software Engineer  Bengaluru, Karnataka, Karnataka - Email me on Indeed: indeed.com/r/Govardhana-K/ b2de315d95905b68  Total IT experience 5 Years 6 Months Cloud Lending Solutions INC 4 Month • Salesforce Developer Oracle 5 Years 2 Month • Core Java Developer Languages Core Java, Go Lang Oracle PL-SQL programming, Sales Force Developer with APEX.  Designations & Promotions  Willing to relocate: Anywhere  WORK EXPERIENCE  Senior Software Engineer  Cloud Lending Solutions -  Bangalore, Karnataka -  January 2018 to Present  Present  Senior Consultant  Oracle -  Bangalore, Karnataka -  November 2016 to December 2017  Staff Consultant  Oracle -  Bangalore, Karnataka -  January 2014 to October 2016  Associate Consultant  Oracle -  Bangalore, Karnataka -  November 2012 to December 2013  EDUCATION  B.E in Computer Science Engineering  Adithya Institute of Technology -  Tamil Nadu  September 2008 to June 2012  https://www.indeed.com/r/Govardhana-K/b2de315d95905b68?isid=rex-download&ikw=download-top&co=IN https://www.indeed.com/r/Govardhana-K/b2de315d95905b68?isid=rex-download&ikw=download-top&co=IN   SKILLS  APEX. (Less than 1 year), Data Structures (3 years), FLEXCUBE (5 years), Oracle (5 years), Algorithms (3 years)  LINKS  https://www.linkedin.com/in/govardhana-k-61024944/  ADDITIONAL INFORMATION  Technical Proficiency:  Languages: Core Java, Go Lang, Data Structures & Algorithms, Oracle PL-SQL programming, Sales Force with APEX. Tools: RADTool, Jdeveloper, NetBeans, Eclipse, SQL developer, PL/SQL Developer, WinSCP, Putty Web Technologies: JavaScript, XML, HTML, Webservice  Operating Systems: Linux, Windows Version control system SVN & Git-Hub Databases: Oracle Middleware: Web logic, OC4J Product FLEXCUBE: Oracle FLEXCUBE Versions 10.x, 11.x and 12.x  https://www.linkedin.com/in/govardhana-k-61024944/', 'entities': [(1749, 1755, 'COMPANIES_WORKED_AT'), (1696, 1702, 'COMPANIES_WORKED_AT'), (1417, 1423, 'COMPANIES_WORKED_AT'), (1356, 1793, 'SKILLS'), (1209, 1215, 'COMPANIES_WORKED_AT'), (1136, 1248, 'SKILLS'), (928, 932, 'GRADUATION_YEAR'), (858, 889, 'COLLEGE_NAME'), (821, 856, 'DEGREE'), (787, 791, 'GRADUATION_YEAR'), (744, 750, 'COMPANIES_WORKED_AT'), (722, 742, 'DESIGNATION'), (658, 664, 'COMPANIES_WORKED_AT'), (640, 656, 'DESIGNATION'), (574, 580, 'COMPANIES_WORKED_AT'), (555, 573, 'DESIGNATION'), (470, 493, 'COMPANIES_WORKED_AT'), (444, 469, 'DESIGNATION'), (308, 314, 'COMPANIES_WORKED_AT'), (234, 240, 'COMPANIES_WORKED_AT'), (175, 198, 'COMPANIES_WORKED_AT'), (93, 137, 'EMAIL_ADDRESS'), (39, 48, 'LOCATION'), (13, 38, 'DESIGNATION'), (0, 12, 'NAME')]}\n"
     ]
    }
   ],
   "source": [
    "training_data = []\n",
    "\n",
    "for d in train_data:\n",
    "    temp_dict = {}\n",
    "    temp_dict[\"text\"] = d[0].strip()\n",
    "    temp_dict[\"entities\"] = []\n",
    "    for entities in d[1][\"entities\"]:\n",
    "        start = entities[0]\n",
    "        end = entities[1]\n",
    "        label = entities[2].upper().strip().replace(\" \", \"_\")\n",
    "        temp_dict[\"entities\"].append((start, end, label))\n",
    "    training_data.append(temp_dict)\n",
    "\n",
    "print(training_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4048, 4169, 'Skills')\n",
      "(4034, 4038, 'Graduation Year')\n",
      "(3977, 4022, 'College Name')\n",
      "(3970, 3975, 'Degree')\n",
      "(3841, 3850, 'Location')\n",
      "(3822, 3837, 'Companies worked at')\n",
      "(3804, 3820, 'Designation')\n",
      "(3755, 3764, 'Location')\n",
      "(3736, 3751, 'Companies worked at')\n",
      "(3721, 3734, 'Designation')\n",
      "(2053, 2062, 'Location')\n",
      "(2034, 2049, 'Companies worked at')\n",
      "(2023, 2032, 'Designation')\n",
      "(493, 497, 'Graduation Year')\n",
      "(452, 467, 'Companies worked at')\n",
      "(217, 226, 'Location')\n",
      "(198, 213, 'Companies worked at')\n",
      "(187, 196, 'Designation')\n",
      "(40, 49, 'Location')\n",
      "(23, 38, 'Companies worked at')\n",
      "(11, 20, 'Designation')\n",
      "(0, 10, 'Name')\n"
     ]
    }
   ],
   "source": [
    "for ent in train_data[9][1][\"entities\"]:\n",
    "    print(ent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp = spacy.blank(\"en\")\n",
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting iteration 0\n",
      "Starting iteration 1\n",
      "Starting iteration 2\n",
      "Starting iteration 3\n",
      "Starting iteration 4\n",
      "Starting iteration 5\n",
      "Starting iteration 6\n",
      "Starting iteration 7\n",
      "Starting iteration 8\n",
      "Starting iteration 9\n",
      "Function train_model took 284.5981 seconds\n"
     ]
    }
   ],
   "source": [
    "@duration\n",
    "def train_model(train_data):\n",
    "\n",
    "    if \"ner\" not in nlp.pipe_names:\n",
    "\n",
    "        ner = nlp.create_pipe(\"ner\")\n",
    "\n",
    "        nlp.add_pipe(ner, last=True)\n",
    "    for (\n",
    "        _,\n",
    "        annotation,\n",
    "    ) in train_data:\n",
    "        for ent in annotation[\"entities\"]:\n",
    "\n",
    "            ner.add_label(ent[2])\n",
    "    other_pipes = [pipe for pipe in nlp.pipe_names if pipe != \"ner\"]\n",
    "\n",
    "    with nlp.disable_pipes(*other_pipes):\n",
    "\n",
    "        optimizer = nlp.begin_training()\n",
    "        for itn in range(EPOCH):\n",
    "            print(\"Starting iteration \" + str(itn))\n",
    "            random.shuffle(train_data)\n",
    "            losses = {}\n",
    "            for text, annotations in train_data:\n",
    "                try:\n",
    "                    nlp.update(\n",
    "                        [text],\n",
    "                        [annotations],\n",
    "                        drop=DROPOUT,\n",
    "                        sgd=optimizer,\n",
    "                        losses=losses,\n",
    "                    )\n",
    "                except Exception as e:\n",
    "                    pass\n",
    "\n",
    "\n",
    "train_model(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.to_disk(CUSTOM_NER_MODEL_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and Test the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ner', <spacy.pipeline.pipes.EntityRecognizer at 0x1fb1b229c48>)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resume_ner_model = spacy.load(CUSTOM_NER_MODEL_PATH)\n",
    "resume_ner_model.pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('College Name',\n",
       " 'Companies worked at',\n",
       " 'Degree',\n",
       " 'Designation',\n",
       " 'Email Address',\n",
       " 'Graduation Year',\n",
       " 'Location',\n",
       " 'Name',\n",
       " 'Skills',\n",
       " 'UNKNOWN',\n",
       " 'Years of Experience')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resume_ner_model.entity.labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SKILLS               - Languages Java Python w framework Pyspark SQL Bash Linux Scala Database and Big Data SQL Oracle MySQL Postgres Hadoop w Hive and Apache Spark ETL Modeling Apache Solr MapReduce PyTorch Tableau Spacy Redis MongoDB and Neo4j database Cloud AWS Certified Solutions Architect Associate Microsoft Azure Azure Data Factory Databricks WORK EXPERIENCE PPG Industries Pittsburgh AI ML Intern May 2022 August 2022 azure pipelines Designed and managed data flows and pipelines in Azure Data Factory for ingestion from relational databases REST APIs Delta Lake Sharepoint and other cloud file storage systems automation Automated the manual process of ingesting Design of Experiment data from workbooks in Sharepoint using PySpark scripts written in Databricks thereby reducing ingestion and curation time by 90 Modak Analytics LLP India Data Engineer September 2019 July 2021 data ingestion Developed PySpark code that minimized ingestion time by 60 to consume terabytes of data from structured and unstructured sources daily using automated bot workflows into the client data lake as Hive tables leading to convenient downstream analysis and data transformation using SQL applications data curation Built complex Spark SQL queries to curate pharmaceutical data on Dataframe collection these were utilized in data analysis and modelling for a elerating the timeline of the drug discovery process by 50 pipeline automation and visualization Conducted the compilation of Python workflows using Subprocess and OS modules which executed Scala scripts to index curated data into Apache Solr and Neo4j graph database for utilization by pharmaceutical SMEs automated workflows would be triggered daily to bring in updated data communication Collaborated with clients and took the initiative to plan strategies to enhance the company s native platform and helped develop creative business requirements for a clinical trials project leadership Spearheaded a team of 5 members who collectively created an efficient data pipeline in PySpark for a client use case that involved crawling ingestion and transformation of terabytes of clinical trials data which resulted in the reduction of original processing time by 50 ACADEMIC PROJECTS Predicting Order Returns March 2022 May 2022 Predicting whether an online order will be returned to an online retailer as a step to increase net margins and improve supply chain inventory management a ording to research this is soon expected to be a trillion dollar problem Initially performing data preparation and feature engineering for downstream modelling and finally predicting the orders which would be returned using Machine Learning classification algorithms such as Decision Tree Random Forest Logistic Regression Naive Bayes and Perceptron Improvement of performance metrics mainly Recall since False Negatives in this case are a bigger concern for a retailer Python Jupyter Notebook Scikit Learn Numpy Plotly Seaborn EDA on H M Transactions Data January 2022 March 2022 Manufactured call to action CTA s for H M regarding production of category colour and pricing using EDA Associating the best channel of sales for the above actionable items Presented the output in a time series format with CTAs for each quarter Sample CTA Quarter 1 Jan March H M should focus on selling lower priced products in the Ladieswear category in Black Blue and Pink colour and sell them through online channels Python Jupyter Notebook Altair Pandas Scipy Numpy Other Projects Travel Planner GUI Recommendation System COMMUNITY SERVICE AND LEADERSHIP EXPERIENCE Live Life Love Life Charity Foundation Fundraising Coordinator October 2017 October 2019 Organized events to spread cancer awareness and raised approximately 40 000 from donations to provide medical assistance for the underprivileged to fight breast cancer Thermax Foundation June 2017 August 2017 Focused on educating underprivileged children in mathematics and science for entrance exams\n"
     ]
    }
   ],
   "source": [
    "doc = resume_ner_model(\n",
    "    \" 412 954 8546 sumedhns SUMEDH SHAH EDUCATION Carnegie Mellon University Pittsburgh Master of Information Systems Management Business Intelligence and Data Analytics GPA 3 96 4 00 Teaching Assistant Unstructured Data Analytics NoSQL Database Management December 2022 Maharashtra Institute of Technology India Bachelor of Engineering in Electronics and Telecommunication Engineering June 2019 GPA 7 79 10 SKILLS Languages Java Python w framework Pyspark SQL Bash Linux Scala Database and Big Data SQL Oracle MySQL Postgres Hadoop w Hive and Apache Spark ETL Modeling Apache Solr MapReduce PyTorch Tableau Spacy Redis MongoDB and Neo4j database Cloud AWS Certified Solutions Architect Associate Microsoft Azure Azure Data Factory Databricks WORK EXPERIENCE PPG Industries Pittsburgh AI ML Intern May 2022 August 2022 azure pipelines Designed and managed data flows and pipelines in Azure Data Factory for ingestion from relational databases REST APIs Delta Lake Sharepoint and other cloud file storage systems automation Automated the manual process of ingesting Design of Experiment data from workbooks in Sharepoint using PySpark scripts written in Databricks thereby reducing ingestion and curation time by 90 Modak Analytics LLP India Data Engineer September 2019 July 2021 data ingestion Developed PySpark code that minimized ingestion time by 60 to consume terabytes of data from structured and unstructured sources daily using automated bot workflows into the client data lake as Hive tables leading to convenient downstream analysis and data transformation using SQL applications data curation Built complex Spark SQL queries to curate pharmaceutical data on Dataframe collection these were utilized in data analysis and modelling for a elerating the timeline of the drug discovery process by 50 pipeline automation and visualization Conducted the compilation of Python workflows using Subprocess and OS modules which executed Scala scripts to index curated data into Apache Solr and Neo4j graph database for utilization by pharmaceutical SMEs automated workflows would be triggered daily to bring in updated data communication Collaborated with clients and took the initiative to plan strategies to enhance the company s native platform and helped develop creative business requirements for a clinical trials project leadership Spearheaded a team of 5 members who collectively created an efficient data pipeline in PySpark for a client use case that involved crawling ingestion and transformation of terabytes of clinical trials data which resulted in the reduction of original processing time by 50 ACADEMIC PROJECTS Predicting Order Returns March 2022 May 2022 Predicting whether an online order will be returned to an online retailer as a step to increase net margins and improve supply chain inventory management a ording to research this is soon expected to be a trillion dollar problem Initially performing data preparation and feature engineering for downstream modelling and finally predicting the orders which would be returned using Machine Learning classification algorithms such as Decision Tree Random Forest Logistic Regression Naive Bayes and Perceptron Improvement of performance metrics mainly Recall since False Negatives in this case are a bigger concern for a retailer Python Jupyter Notebook Scikit Learn Numpy Plotly Seaborn EDA on H M Transactions Data January 2022 March 2022 Manufactured call to action CTA s for H M regarding production of category colour and pricing using EDA Associating the best channel of sales for the above actionable items Presented the output in a time series format with CTAs for each quarter Sample CTA Quarter 1 Jan March H M should focus on selling lower priced products in the Ladieswear category in Black Blue and Pink colour and sell them through online channels Python Jupyter Notebook Altair Pandas Scipy Numpy Other Projects Travel Planner GUI Recommendation System COMMUNITY SERVICE AND LEADERSHIP EXPERIENCE Live Life Love Life Charity Foundation Fundraising Coordinator October 2017 October 2019 Organized events to spread cancer awareness and raised approximately 40 000 from donations to provide medical assistance for the underprivileged to fight breast cancer Thermax Foundation June 2017 August 2017 Focused on educating underprivileged children in mathematics and science for entrance exams \"\n",
    ")\n",
    "for ent in doc.ents:\n",
    "    print(f\"{ent.label_.upper():{20}} - {ent.text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mined-hack-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
